
## cmd example
##python data_preprocess.py \
##  --input ../elder_care_teachered_softlabels.json \
##  --output ./output.json

import json
import numpy as np
import argparse
import os
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
import codecs

# å›ºå®šç‰¹å¾é¡ºåº
FEATURE_NAMES = [
    'age', 'bmi', 'heart_rate', 'systolic_bp', 'diastolic_bp',
    'o2_saturation', 'steps_today', 'inactivity_duration',
    'egfr', 'is_voice_slow', 'is_night_walking'
]

# è½¯æ ‡ç­¾ç±»åˆ«
STATE_LABELS = [
    "æ­£å¸¸", "ç–²åŠ³", "å¿ƒç‡å¼‚å¸¸", "è¡€å‹åé«˜",
    "è¡€æ°§åä½", "æƒ…ç»ªä½è½", "è·Œå€’é£é™©ä¸­", "è®¤çŸ¥æ¨¡ç³Šæ—©æœŸå¾å…†"
]


def load_json_data(file_path):
    """å¥å£®åŠ è½½ JSON æ•°æ®ï¼Œæ”¯æŒï¼šæ¯è¡Œä¸€ä¸ª JSON"""
    data = []
    errors = 0

    # ä½¿ç”¨ codecs é¿å… BOM é—®é¢˜
    with codecs.open(file_path, 'r', encoding='utf-8-sig') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue

            try:
                sample = json.loads(line)
                data.append(sample)

            except json.JSONDecodeError as e:
                if '}' in line and '{' in line[1:]:
                    print(f"ğŸŸ¡ ç¬¬ {line_num} è¡Œç–‘ä¼¼å¤šä¸ª JSON æ‹¼æ¥ï¼Œå°è¯•åˆ†å‰²ï¼š{line[:50]}...")
                    parts = line.split('}{')
                    for i, part in enumerate(parts):
                        if i == 0:
                            part += '}'
                        elif i == len(parts) - 1:
                            part = '{' + part
                        else:
                            part = '{' + part + '}'

                        try:
                            sample = json.loads(part)
                            data.append(sample)
                        except:
                            errors += 1
                            continue
                else:
                    errors += 1
                    if errors < 10:
                        print(f"âŒ ç¬¬ {line_num} è¡Œ JSON è§£æé”™è¯¯: {e}ï¼Œè·³è¿‡: {line[:50]}")
    print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡æ ·æœ¬ï¼Œè·³è¿‡ {errors} æ¡é”™è¯¯")
    return data


def extract_features_and_labels(data):
    """æå–ç‰¹å¾å’Œè½¯æ ‡ç­¾"""
    X_list = []
    Y_list = []

    for i, sample in enumerate(data):
        try:
            x = [sample[feat] for feat in FEATURE_NAMES]
            X_list.append(x)

            if isinstance(sample.get('teacher_output'), list):
                y = sample['teacher_output']
            elif isinstance(sample.get('teacher_output'), dict):
                y = [sample['teacher_output'].get(state, 0.0) for state in STATE_LABELS]
            else:
                y = [0.0] * len(STATE_LABELS)
            Y_list.append(y)

        except Exception as e:
            print(f"âš ï¸ å¤„ç†ç¬¬ {i+1} ä¸ªæ ·æœ¬å¤±è´¥: {e}ï¼Œæ•°æ®: {sample}")
            continue

    return np.array(X_list), np.array(Y_list)


def normalize_features(X, method='standard'):
    """å½’ä¸€åŒ–ç‰¹å¾"""
    if X.size == 0:
        raise ValueError("âŒ å½’ä¸€åŒ–å¤±è´¥ï¼šX ä¸ºç©º")

    if method == 'standard':
        scaler = StandardScaler()
    elif method == 'minmax':
        scaler = MinMaxScaler()
    elif method == 'robust':
        scaler = RobustScaler()
    else:
        return X, {"method": "none"}

    X_scaled = scaler.fit_transform(X)

    scaler_info = {
        "method": method,
        "mean": scaler.mean_.tolist(),
        "scale": scaler.scale_.tolist(),
        "min": getattr(scaler, 'data_min_', None).tolist() if method == 'minmax' else None,
        "max": getattr(scaler, 'data_max_', None).tolist() if method == 'minmax' else None
    }

    return X_scaled, scaler_info


def main(input_file, output_dir, normalize_method):
    os.makedirs(output_dir, exist_ok=True)

    print("ğŸ” æ­£åœ¨åŠ è½½æ•°æ®...")
    data = load_json_data(input_file)

    if len(data) == 0:
        raise ValueError("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼šæ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ ¼å¼ï¼")

    print("ğŸ“Š æ­£åœ¨æå–ç‰¹å¾å’Œè½¯æ ‡ç­¾...")
    X, Y = extract_features_and_labels(data)

    if X.shape[0] == 0:
        raise ValueError("âŒ ç‰¹å¾æå–å¤±è´¥ï¼šæ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ï¼Œè¯·æ£€æŸ¥å­—æ®µåæ˜¯å¦åŒ¹é…ï¼")

    print(f"âœ… æ„å»ºè¾“å…¥ç‰¹å¾çŸ©é˜µ: {X.shape}")
    print(f"âœ… æ„å»ºè½¯æ ‡ç­¾çŸ©é˜µ: {Y.shape}")

    if normalize_method and normalize_method != 'none':
        print(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨ {normalize_method} æ–¹æ³•å½’ä¸€åŒ–ç‰¹å¾...")
        X, scaler_info = normalize_features(X, method=normalize_method)
        with open(f"{output_dir}/scaler_info.json", 'w', encoding='utf-8') as f:
            json.dump(scaler_info, f, ensure_ascii=False, indent=2)
        print(f"âœ… å½’ä¸€åŒ–å‚æ•°å·²ä¿å­˜: {output_dir}/scaler_info.json")
    else:
        scaler_info = {"method": "none"}

    np.save(f"{output_dir}/X_features.npy", X)
    np.save(f"{output_dir}/Y_soft_labels.npy", Y)
    print(f"âœ… ç‰¹å¾çŸ©é˜µå·²ä¿å­˜: {output_dir}/X_features.npy")
    print(f"âœ… è½¯æ ‡ç­¾çŸ©é˜µå·²ä¿å­˜: {output_dir}/Y_soft_labels.npy")

    with open(f"{output_dir}/feature_names.json", 'w', encoding='utf-8') as f:
        json.dump(FEATURE_NAMES, f, ensure_ascii=False, indent=2)

    print("ğŸ‰ æ•°æ®é¢„å¤„ç†å®Œæˆï¼")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="åº·å…»çŸ¥è¯†è’¸é¦æ•°æ®é¢„å¤„ç†")
    parser.add_argument("--input", type=str, required=True, help="è¾“å…¥ JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", type=str, default="./student_data", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--normalize", type=str, default="standard",
                        choices=["standard", "minmax", "robust", "none"], help="å½’ä¸€åŒ–æ–¹æ³•")

    args = parser.parse_args()
    main(args.input, args.output_dir, args.normalize if args.normalize != 'none' else None)